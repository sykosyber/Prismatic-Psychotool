# app.py

import os
import openai
from flask import Flask, request, jsonify
from flask_cors import CORS  # 1. MAKE SURE THIS IMPORT IS HERE
from dotenv import load_dotenv
import time
import asyncio
# Import your prompts
from prompts import SYSTEM_PROMPT, LENS_PROMPTS, SYNTHESIS_PROMPT

# --- INITIALIZATION ---
load_dotenv()
app = Flask(__name__)
CORS(app)  

# Check if Mock Mode is enabled from the .env file
MOCK_MODE = os.environ.get("MOCK_MODE", "False").lower() in ['true', '1', 't']

# Use AsyncOpenAI for compatibility with async routes
client = openai.AsyncOpenAI(api_key=os.environ.get("OPENAI_API_KEY"))


# --- HELPER FUNCTIONS ---

def format_as_html(text_content: str) -> str:
    """Formats the raw AI response into a structured HTML string."""
    paragraphs = text_content.strip().split('\n\n')
    html_output = "".join(f"<p>{p.strip()}</p>" for p in paragraphs if p.strip())
    return html_output

async def get_ai_analysis(user_prompt: str, max_tokens: int) -> str:
    """
    Calls the OpenAI Chat Completions API and returns the text content.
    """
   # ================= MOCK MODE LOGIC =================
    if MOCK_MODE:
        print("--- MOCK MODE ENABLED: Returning dummy data. No API call was made. ---")
        await asyncio.sleep(2)  # Simulate a 2-second API delay to test your loading spinner

        mock_response = (
            "This is a mock analysis generated by the backend. "
            "It confirms that the frontend is successfully communicating with the server. "
            "The selected lens and user input would be processed here in a live environment.\n\n"
            "Key Point 1: The system's 'plumbing' is working correctly.\n"
            "Key Point 2: You can now build your entire UI against this predictable response without spending any API credits."
        )
        return mock_response
    # ===================================================
   
   
    try:
        response = await client.chat.completions.create(
            model="gpt-4o",  # Or "gpt-4-turbo", "gpt-3.5-turbo"
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt}
            ],
            max_tokens=max_tokens,
            temperature=0.5, # Adjust for more or less creative responses
        )
        # --- FIX IS HERE ---
        content = response.choices[0].message.content
        if content is None:
            # Handle the case where the API returns no content
            raise ValueError("API returned an empty message content.")
        
        return content
        # --- END FIX ---
    except Exception as e:
        print(f"Error calling OpenAI API: {e}")
        return "Error: Could not retrieve analysis from the AI model."


# --- API ENDPOINT ---

# This route remains the same as the Anthropic version.
@app.route("/api/analyze", methods=["POST"])
async def handle_analysis():
    data = request.get_json()
    if not data:
        return jsonify({"error": "Invalid JSON"}), 400

    user_text = data.get("userInput")
    selected_lenses = data.get("selectedLenses", [])
    include_synthesis = data.get("includeIntegration", False)

    all_analyses = {}

    # Perform analysis for each selected lens
    for lens in selected_lenses:
        lens_id = lens.get("id")
        if lens_id in LENS_PROMPTS:
            prompt_template = LENS_PROMPTS[lens_id]
            full_prompt = f"{prompt_template}\n\nINPUT TEXT:\n\"\"\"\n{user_text}\n\"\"\""

            analysis_text = await get_ai_analysis(full_prompt, 1500)
            all_analyses[lens_id] = {
                "name": lens.get("name"),
                "content": format_as_html(analysis_text)
            }

    # Perform synthesis if requested
    synthesis_result = None
    if include_synthesis and len(all_analyses) > 1:
        # This logic remains the same
        combined_analyses_text = "\n\n---\n\n".join(
            f"Analysis from {details['name']}:\n{details['content']}"
            for _, details in all_analyses.items()
        )
        synthesis_prompt_filled = SYNTHESIS_PROMPT.format(
            combined_analyses=combined_analyses_text,
            user_input=user_text
        )
        synthesis_text = await get_ai_analysis(synthesis_prompt_filled, 2000)
        synthesis_result = format_as_html(synthesis_text)

    return jsonify({
        "analyses": all_analyses,
        "synthesis": synthesis_result
    }), 200


# --- RUN THE APP ---

if __name__ == "__main__":
    # For production, use a proper WSGI server like Gunicorn
    app.run(debug=True, port=5000)